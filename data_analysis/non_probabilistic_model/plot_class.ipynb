{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sqlite3\n",
    "\n",
    "# Load the true positive predictions from the CSV file\n",
    "conn = sqlite3.connect('../../data/nfp2/nfp2.db')  # Adjust the path to your database file\n",
    "\n",
    "# Step 2 & 3: Query the database and load the data into a pandas DataFrame\n",
    "query = \"SELECT * FROM stellarators\"  # Adjust your query as needed\n",
    "data_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "data_df_clean = data_df[data_df['convergence'] == 1]\n",
    "data_df_clean = data_df_clean.dropna(subset=['quasisymmetry'])\n",
    "\n",
    "\n",
    "X = data_df_clean[['rbc_1_0', 'rbc_m1_1', 'rbc_0_1', 'rbc_1_1','zbs_1_0', 'zbs_m1_1', 'zbs_0_1', 'zbs_1_1']] \n",
    "Y = np.log(data_df_clean['quasisymmetry'])\n",
    "\n",
    "\n",
    "# Print NaN counts in the input features\n",
    "print(\"NaN counts in input features:\")\n",
    "print(X.isna().sum())\n",
    "\n",
    "# Print NaN counts in the target variable\n",
    "print(\"NaN counts in target variable:\")\n",
    "print(Y.isna().sum())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_no_outliers, test_features_no_outliers, target_no_outliers, test_target_no_outliers = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#print('Best trial:', study.best_trial)\n",
    "#print('Best value:', study.best_value)\n",
    "#print('Best parameters:', study.best_params)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Assuming study.best_params already includes the best hyperparameters from your Optuna study for a regression problem\n",
    "#model = lgb.LGBMRegressor(**study.best_params)\n",
    "\n",
    "best_params_manual = {\n",
    "    \"boosting_type\": \"dart\",\n",
    "    \"max_depth\": 35,\n",
    "    \"num_leaves\": 449,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"feature_fraction\": 0.9824756723113014,\n",
    "    \"learning_rate\": 0.24479713555351562,\n",
    "    \"num_iterations\": 2917,\n",
    "    \"data_sample_strategy\": \"bagging\",\n",
    "    \"max_bins\": 1236\n",
    "}\n",
    "\n",
    "# Create and train the LightGBM model with the manually defined best parameters\n",
    "model = lgb.LGBMRegressor(**best_params_manual)\n",
    "\n",
    "# Assuming features_no_outliers and target_no_outliers are your feature matrix and target vector, respectively\n",
    "model.fit(features_no_outliers, target_no_outliers)\n",
    "\n",
    "# After fitting, you can use the model to predict or evaluate it further\n",
    "# For example, to predict new values\n",
    "predictions = model.predict(test_features_no_outliers)\n",
    "\n",
    "mse = mean_squared_error(test_target_no_outliers, predictions)\n",
    "mae = mean_absolute_error(test_target_no_outliers, predictions)\n",
    "r2 = r2_score(test_target_no_outliers, predictions)\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test R^2: {r2}\")\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"Predicted\": predictions.flatten(),  # Flatten in case the predictions are in a 2D array\n",
    "    \"Type\": \"Predicted\"\n",
    "})\n",
    "df_actual = pd.DataFrame({\n",
    "    \"Predicted\": np.tile(test_target_no_outliers, (len(predictions) // len(test_target_no_outliers))),\n",
    "    \"Type\": \"Actual\"\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] < 0:\n",
    "        print(f\"Predicted: {predictions[i]}, Actual: {test_target_no_outliers.iloc[i]}\")\n",
    "\n",
    "# Combine and plot\n",
    "#df_combined = pd.concat([df_predictions, df_actual])\n",
    "print(predictions.shape)\n",
    "print(test_target_no_outliers.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(predictions, fill=True, color=\"blue\", alpha=0.5, label=\"Predicted\")\n",
    "sns.kdeplot(test_target_no_outliers, fill=True, color=\"orange\", alpha=0.5, label=\"Actual\")\n",
    "plt.title('Density Plot of Predicted Outputs vs Actual Values')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=10)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
