{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('../data/nfp2/nfp2.db')  # Adjust the path to your database file\n",
    "\n",
    "# Step 2 & 3: Query the database and load the data into a pandas DataFrame\n",
    "query = \"SELECT * FROM stellarators\"  # Adjust your query as needed\n",
    "data_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "data_df_clean = data_df.dropna(subset=['quasisymmetry'])\n",
    "\n",
    "X = data_df_clean[['rbc_1_0', 'rbc_m1_1', 'rbc_0_1', 'rbc_1_1','zbs_1_0', 'zbs_m1_1', 'zbs_0_1', 'zbs_1_1']] \n",
    "Y = data_df_clean[['quasisymmetry', 'quasiisodynamic', 'rotational_transform', 'inverse_aspect_ratio', 'mean_local_magnetic_shear', 'vacuum_magnetic_well', 'maximum_elongation', 'mirror_ratio']]\n",
    "\n",
    "target = Y['quasisymmetry']\n",
    "features = X\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "print(features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.kdeplot(np.log(target), bw_adjust=0.5)\n",
    "plt.title('Density Function')\n",
    "plt.xlabel('Log(quasisymmetry)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming Y_train, X_train, Y_test, and X_test are pandas Series/DataFrames\n",
    "\n",
    "# Calculate the IQR and bounds for outliers\n",
    "q1 = Y_train.quantile(0.05)\n",
    "q3 = Y_train.quantile(0.95) \n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Filter out the outliers from Y_train\n",
    "target_no_outliers = Y_train[(Y_train >= lower_bound) & (Y_train <= upper_bound)]\n",
    "\n",
    "# Check and filter X_train based on the indices of the filtered Y_train\n",
    "features_no_outliers = X_train.loc[target_no_outliers.index.intersection(X_train.index)]\n",
    "\n",
    "# For X_test and Y_test, you need to apply a similar filter or ensure the indices match\n",
    "# Assuming Y_test should be filtered using the same bounds defined by Y_train\n",
    "test_target_no_outliers = Y_test[(Y_test >= lower_bound) & (Y_test <= upper_bound)]\n",
    "test_features_no_outliers = X_test.loc[test_target_no_outliers.index.intersection(X_test.index)]\n",
    "\n",
    "# Plot the KDE of log-transformed target values without outliers\n",
    "sns.kdeplot(np.log(target_no_outliers), bw_adjust=0.5)\n",
    "plt.title('Density Function')\n",
    "plt.xlabel('Log(quasisymmetry)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(features_no_outliers, target_no_outliers, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Assuming `Y_train_np` is your 1D numpy array data\n",
    "Y_train_np = Y_train.to_numpy().reshape(-1, 1)  # Reshape to 2D if necessary\n",
    "\n",
    "# Use AIC and BIC to determine the best number of components for GMM\n",
    "n_components_range = range(1, 10)  # Example range, can be adjusted\n",
    "lowest_bic = np.infty\n",
    "lowest_aic = np.infty\n",
    "best_gmm = None\n",
    "bic = []\n",
    "aic = []\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    # Fit a Gaussian mixture with n components\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "    gmm.fit(Y_train_np)\n",
    "    \n",
    "    # Calculate the BIC and AIC\n",
    "    bic.append(gmm.bic(Y_train_np))\n",
    "    aic.append(gmm.aic(Y_train_np))\n",
    "    \n",
    "    # Determine if this model has the lowest BIC\n",
    "    if bic[-1] < lowest_bic:\n",
    "        lowest_bic = bic[-1]\n",
    "        best_gmm_bic = gmm\n",
    "        \n",
    "    # Determine if this model has the lowest AIC\n",
    "    if aic[-1] < lowest_aic:\n",
    "        lowest_aic = aic[-1]\n",
    "        best_gmm_aic = gmm\n",
    "\n",
    "# Plot the BIC scores\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(n_components_range, bic, label='BIC')\n",
    "plt.plot(n_components_range, aic, label='AIC')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Criterion Value')\n",
    "plt.title('BIC and AIC for GMM')\n",
    "plt.show()\n",
    "\n",
    "# Use the best model for the final density plot\n",
    "best_gmm = best_gmm_bic if lowest_bic < lowest_aic else best_gmm_aic\n",
    "print(f\"Selected Model Components: {best_gmm.n_components}\")\n",
    "\n",
    "# Generate some data points for plotting the density\n",
    "x = np.linspace(Y_train_np.min(), Y_train_np.max(), 1000).reshape(-1, 1)\n",
    "logprob = best_gmm.score_samples(x)\n",
    "responsibilities = best_gmm.predict_proba(x)\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "# Plot the actual kernel density estimation of your data\n",
    "sns.kdeplot(Y_train_np.flatten(), bw_adjust=0.5, label='Actual')\n",
    "\n",
    "# Plot each component density\n",
    "for i in range(best_gmm.n_components):\n",
    "    plt.plot(x, pdf_individual[:, i], label=f'Gaussian {i+1}')\n",
    "\n",
    "# Plot the total density\n",
    "plt.plot(x, pdf, label='Mixture Total')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Actual data density vs. Gaussian Mixture Model density')\n",
    "plt.xlabel('Data values')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbmlss.distributions import *\n",
    "from lightgbmlss.distributions.distribution_utils import DistributionClass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Your data preparation steps\n",
    "Y_train_np = np.array(Y_train)\n",
    "\n",
    "lgblss_dist_class = DistributionClass()\n",
    "candidate_distributions = [Gaussian, StudentT, Gamma, Cauchy, LogNormal, Weibull, Gumbel, Laplace]\n",
    "\n",
    "# Selecting the best distribution based on negative log-likelihood\n",
    "dist_nll = lgblss_dist_class.dist_select(target=Y_train_np, candidate_distributions=candidate_distributions, max_iter=50, plot=True, figure_size=(8, 4))\n",
    "dist_nll\n",
    "\n",
    "# Plot the actual data density\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.kdeplot(Y_train_np, bw_adjust=0.5, label='Actual Data Density')\n",
    "plt.title('Density Function of Target Data')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "este grafico apenas ve a distribuição da target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "%matplotlib inline\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 25),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 50),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 300),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_boost_round': trial.suggest_int('num_boost_round', 100, 1000)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        Y_train_fold, Y_valid_fold = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "        \n",
    "        gbm = lgb.LGBMRegressor(**param)\n",
    "        gbm.fit(X_train_fold, Y_train_fold, eval_set=[(X_valid_fold, Y_valid_fold)], eval_metric='mse',\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "        preds = gbm.predict(X_valid_fold)\n",
    "        mse_scores.append(mean_squared_error(Y_valid_fold, preds))\n",
    "    \n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "# Set TPESampler as the sampler algorithm\n",
    "sampler = TPESampler()\n",
    "\n",
    "# Create a study object and specify the optimization direction (minimize)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler, pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Run the optimization with TPESampler as the sampler\n",
    "study.optimize(objective, n_trials=50, gc_after_trial=True)\n",
    "\n",
    "# Write results to a file\n",
    "with open('/home/rofarate/PIC-STELLARATOR/data_analysis/optuna_trials.txt', 'w') as f:\n",
    "    f.write(f\"Best Parameters: {study.best_params}\\n\")\n",
    "    f.write(f\"Best Score: {study.best_value}\\n\")\n",
    "\n",
    "    # Optionally, write all trial results\n",
    "    for trial in study.trials:\n",
    "        f.write(f\"Trial {trial.number}, Value: {trial.value}, Params: {trial.params}\\n\")\n",
    "\n",
    "\n",
    "# Access the best parameters and best score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Best trial:', study.best_trial)\n",
    "#print('Best value:', study.best_value)\n",
    "#print('Best parameters:', study.best_params)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Assuming study.best_params already includes the best hyperparameters from your Optuna study for a regression problem\n",
    "#model = lgb.LGBMRegressor(**study.best_params)\n",
    "\n",
    "best_params_manual = {\n",
    "    \"max_depth\": 28,\n",
    "    \"num_leaves\": 218,\n",
    "    \"min_data_in_leaf\": 183,\n",
    "    \"feature_fraction\": 0.8597289506168069,\n",
    "    \"learning_rate\": 0.05415891117304222,\n",
    "    \"num_iterations\": 718,\n",
    "    \"data_sample_strategy\": \"bagging\",\n",
    "    \"max_bins\": 828,\n",
    "    \"boosting_type\": \"gbdt\"\n",
    "}\n",
    "\n",
    "# Create and train the LightGBM model with the manually defined best parameters\n",
    "model = lgb.LGBMRegressor(**best_params_manual)\n",
    "\n",
    "# Assuming features_no_outliers and target_no_outliers are your feature matrix and target vector, respectively\n",
    "model.fit(features_no_outliers, target_no_outliers)\n",
    "\n",
    "# After fitting, you can use the model to predict or evaluate it further\n",
    "# For example, to predict new values\n",
    "predictions = model.predict(test_features_no_outliers)\n",
    "\n",
    "mse = mean_squared_error(test_target_no_outliers, predictions)\n",
    "mae = mean_absolute_error(test_target_no_outliers, predictions)\n",
    "r2 = r2_score(test_target_no_outliers, predictions)\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test R^2: {r2}\")\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"Predicted\": predictions.flatten(),  # Flatten in case the predictions are in a 2D array\n",
    "    \"Type\": \"Predicted\"\n",
    "})\n",
    "df_actual = pd.DataFrame({\n",
    "    \"Predicted\": np.tile(test_target_no_outliers, (len(predictions) // len(test_target_no_outliers))),\n",
    "    \"Type\": \"Actual\"\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine and plot\n",
    "#df_combined = pd.concat([df_predictions, df_actual])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(predictions.flatten(), fill=True, color=\"blue\", alpha=0.5, label=\"Predicted\")\n",
    "sns.kdeplot(test_target_no_outliers, fill=True, color=\"orange\", alpha=0.5, label=\"Actual\")\n",
    "plt.title('Density Plot of Predicted Outputs vs Actual Values')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = \"lightgbm.pkl\"\n",
    "pickle.dump(model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP WITH FEATURES AND MEAN ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP WITH FEATURES AND STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP WITH TARGET AND MEAN ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP WITH TARGET AND STD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
